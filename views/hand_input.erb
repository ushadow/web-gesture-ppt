<!DOCTYPE html>
<html>
  <head>
    <title>Kinect Hand Input</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, 
    maximum-scale=1.0, user-scalable=no"/>
    <link rel="stylesheet" href="application.css" type="text/css" />
    <link rel="stylesheet" href="css/reveal.min.css"/>
    <link rel="stylesheet" href="css/theme/default.css" id="theme"/>
    <script src="js/jquery.js" type="text/javascript"></script>
    <script src="application.js" type="text/javascript"></script>
  </head>
  <body>
    <div class="reveal">
      <button>Connect</button>
      <input id="ws-addr" type="text" value="127.0.0.1:8080"/>
      <label id="status-label" class="status">Status:</label>
      <span id="status-text"></span>
      <div class="slides">
        <section>
        <h1>Real-time Gesture Recognition for Natural Multimodal 
          Interaction</h1>
        <h2>Ying Yin</h2>
        </section>
        <section>
        <video width="1280" height="720" muted controls loop data-autoplay>
        <source src="media/ironman.mp4" type="video/mp4"> 
        </video>
        </section>
        <section>
          <section>
            <h2>Gesture Taxonomy</h2>
            <ul>
              <li>Form
              <ul>
                <li>path</li>
                <li>pose</li>
              </ul>
              </li>
              <li>Flow
              <ul>
                <li>discrete</li>
                <li>continuous</li>
              </ul>
              </li>
            </ul>
          </section>
          <section>
          <h2>Temporal Model of Gestures</h2>
          <p>Pre-stroke -&gt; Nucleus -&gt; Post-stroke</p>
          <video width="1280" height="720" muted controls loop data-autoplay>
          <source src="media/gesture_temporal.mp4" type="video/mp4"> 
          </video>
          </section>
        </section>
        <section>
        <h2>Contributions</h2>
        <ul>
          <li>Improved hand tracking using gesture salience</li>
          <li>Handle different <em>forms</em> of gestures under one probabilistic framework</li>
          <li>Use hidden states to identify gesture phases</li>
            <ul>
              <li>Alow system to respond more promptly and appropriately according to gesture <em>flow</em></li>
              <li>Gesture spotting</li>
            </ul>
          <li>Real-time online inference on flattened HMM</li>
          <li>Collected a new dataset with two <em>forms</em> of gestures</li>
          <li>Developed a hybrid evaluation metric more relevant to real-time interaction</li>
        </ul>
        </section>
        <section>
        <h2>System Overview</h2>
        <img src="media/system_overview.jpg"/>
        </section>
        <section>
        <h2>Hand Tracking</h2>
        </section>
      </div>
    </div>
    <script src="js/reveal.min.js"></script>
    <script src="js/head.min.js"></script>
    <script>
      Reveal.initialize({
        gesture: {
          mirror: false
        }
      });
    </script>
  </body>
</html>

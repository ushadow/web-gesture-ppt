<!DOCTYPE html>
<html>
  <head>
    <title>Kinect Hand Input</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, 
    maximum-scale=1.0, user-scalable=no"/>
    <link rel="stylesheet" href="application.css" type="text/css" />
    <link rel="stylesheet" href="css/reveal.css"/>
    <link rel="stylesheet" href="css/theme/simple.css" id="theme"/>
    <script src="js/jquery.js" type="text/javascript"></script>
    <script src="application.js" type="text/javascript"></script>
  </head>
  <body>
    <div class="reveal">
      <button>Connect</button>
      <input id="ws-addr" type="text" hidden value="127.0.0.1:8080"/>
      <span id="status-text"></span>
      <div class="slides">
        <section>
          <h1>Real-time Gesture Recognition for Natural Multimodal 
            Interaction</h1>
          <h2>Ying Yin</h2>
        </section>
        <section>
          <video width="761" controls muted preload="auto">
          <source src="media/video/kinect.mp4" type="video/mp4">
          </video>
        </section>
        <section>
          <video width="1280" height="720" muted controls preload="auto">
          <source src="media/video/ironman_short.mp4" type="video/mp4"> 
          </video>
        </section>
        <section data-transition="none">
          <h2>Gesture Taxonomy</h2>
          <img src="media/taxonomy.jpg"/>
        </section>
        <section data-transition="none">
          <h2>Gesture Taxonomy</h2>
          <img src="media/taxonomy1.jpg"/>
        </section>
        <section data-transition="none">
          <h2>Gesture Taxonomy</h2>
          <img src="media/taxonomy_circle.jpg"/>
        </section>
        <section data-transition="none">
          <h2>Gesture Taxonomy</h2>
          <img src="media/taxonomy_point.jpg"/>
        </section>
        <section data-transition="none">
          <h2>Gesture Taxonomy</h2>
          <img src="media/taxonomy2.jpg"/>
        </section>
        <section data-transition="none">
          <section data-transition="none">
          <h2>Gesture Taxonomy</h2>
            <img src="media/taxonomy_all.jpg"/>
          </section>
          <section>
            <video width="911" muted controls preload="auto" data-autoplay>
            <source src="media/video/plane.mp4" type="video/mp4"> 
            </video>
          </section>
        </section>
        <section>
          <h2>Temporal Model of Gestures</h2>
          <h3>Pre-stroke -&gt; Nucleus -&gt; Post-stroke</h3>
          <video width="610" muted controls preload="auto">
          <source src="media/video/gesture_temporal.mp4" type="video/mp4"> 
          </video>
        </section>
        <section>
        <h2>Contributions</h2>
        <ul>
          <li><h3>Human computer interaction</h3>
            <ul class="fragment">
              <li>Smoothly handles <em>path</em> and <em>pose</em> gestures</li>
              <li>Responds to <em>discrete</em> and <em>continous</em> flow gestures appropriately and promptly</li>
            </ul>
          </li>
          <li><h3>Machine learning</h3>
            <ul class="fragment">
              <li>Probabilistic model that unifies two forms of gestures</li>
              <li>Hidden state -&gt; gesture phase -&gt; response timing</li>
              <li>Achieves balance between accuracy and responsiveness</li>
            </u>
          </li>
        </ul>
        </section>
        <section>
          <section data-transition="none">
            <h2>System Overview</h2>
            <img src="media/system_overview.jpg"/>
          </section>
          <section data-transition="none">
            <h2>System Overview</h2>
            <img src="media/system_overview_tracking.jpg"/>
          </section>
        </section>
        <section>
          <section>
            <h2>Horizontal Orientation</h2>
            <img src="media/tabletop.jpg"/>
          </section>
          <section data-transition="none">
            <img src="media/handtracking.JPG"/>
          </section>
          <section data-transition="none">
            <video muted controls data-autoplay>
            <source src="media/video/pointing.mp4" type="video/mp4">
            </video>
          </section>
        </section>
        <section>
          <section data-transition="none">
            <h2>Vertical Orientation</h2>
            <h3>Salience Detection for Gesture [ICMI'13]</h3>
            <img src="media/gesture_salience.png"/>
          </section>
          <section data-transition="none">
            <h2>Vertical Orientation</h2>
            <h3>Salience Detection for Gesture [ICMI'13]</h3>
            <video data-autoplay muted controls>
            <source src="media/video/chairgest_handtracking.mp4" type="video/mp4">
            </video>
          </section>
        </section>
        <section>
          <section>
            <img src="media/system_overview_feature.jpg"/>
          </section>
          <section>
            <h2>Features</h2>
            <ul>
              <li>Motion features
                <ul>
                  <li>Relative position, velocity, acceleration</li>
                </ul>
              </li>
              <li>Hand pose features
                <ul>
                  <li>Histogram of oriented gradients (HOG) from depth data</li>
                  <li>Principal component analysis (PCA) dimension reduction</li>
                </ul>
              </li>
            </ul>
            <p></p>
          </section>
          <section>
            <img src="media/hog.jpg"/>
          </section>
        </section>
        <section>
          <section>
            <img src="media/system_overview_recognition.jpg"/>
          </section>
          <section>
            <h2>Real-time  Gesture Recognition</h2>
            <ul>
              <li>Model: <em>unified</em> probabilistic model</li>
              <li>Training: learn parameters by examples</li>
              <li>Inference: online <em>simultaneous</em> segmentation and recognition</li>
            </ul>
          </section>
          <section>
            <h2>Why Unified Model</h2>
            <ul>
              <li>Previous work
                <ul>
                  <li><em>Path</em> gestures: HMM or HCRF</li>
                  <li><em>Pose</em> gestures: Nearest Neighbor and SVM</li>
                </ul>
              </li>
              <li class="fragment">Possible approaches to combine both 
                <ul>
                  <li>Classify form first and use different methods 
                    <ul>
                      <li class="fragment"><em class="highlight-red">Make early decistions that are hard to correct later</em></li>
                    </ul>
                  </li>
                  <li class="fragment">Apply different methods concurrently 
                  and compare probabilities
                    <ul>
                      <li class="fragment"><em class="highlight-red">How to compare probabilities from two different models?</em></li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li class="fragment">Unified probabilistic model 
                based on hierarchical HMM
                <ul>
                  <li>Comparable probabilities</li>
                  <li>Different topologies for different <em>forms</em></li>
                  <li>Make soft decisions and propagate probabilities
                  <li>Until a response is required according to gesture <em>flow</em></li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
        <section>
          <section data-transition="none">
          <h2>Hierarchical HMM</h2>
          <h3>State Transition Diagram</h3>
          <img src="media/phase_hmm.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Hierarchical HMM</h2>
          <h3>State Transition Diagram</h3>
          <img src="media/combined.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Hierarchical HMM</h2>
          <h3>State Transition Diagram</h3>
          <img src="media/phase_hhmm_obs.jpg"/>
          </section>
        </section>
        <section>
          <h2>Path and Pose Gestures</h2>
          <ul>
            <li>Different topologies</li>
            <li>Different training strategies</li>
          </ul>
        </section>
        <section>
          <section data-transition="none">
            <h2>Path Gestures</h2>
            <img src="media/path.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Path Gestures</h2>
            <img src="media/embedded.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Path Gestures</h2>
            <img src="media/embedded_prob.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Feature Distributions - Hand Pose</h2>
            <img src="media/hist_pca.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Feature Distributions - Motion</h2>
            <img src="media/motion_hist.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Path Gestures</h2>
            <img src="media/embedded_gmm.jpg"/>
          </section>
        </section>
        <section>
          <section>
            <h2>Path Gesture</h2>
            <ul>
              <li>Training strategies</li>
              <ul>
                <li>Embedded training</li>
                <li>Two-pass training
                  <ol>
                    <li>Viterbi alignment</li>
                    <li>Baum-Welch expectation maximization</li>
                  </ol>
                </li>
              </ul>
            </ul>
          </section>
          <section data-transition="none">
            <img src="media/training1.jpg"/>
            <small>Graphical Model Representation of HMM</small>
          </section>
          <section data-transition="none">
            <img src="media/training2.jpg"/>
            <small>Graphical Model Representation of HMM</small>
          </section>
          <section data-transition="none">
            <img src="media/training3.jpg"/>
            <small>Graphical Model Representation of HMM</small>
          </section>
          <section data-transition="none">
            <h2>Path Gesture</h2>
            <ul>
              <li>Training strategies</li>
              <ul>
                <li>Embedded training</li>
                <li>Two-pass training
                  <ol>
                    <li>Viterbi alignment</li>
                    <li>Baum-Welch expectation maximization
                      <ul>
                        <li>Maximum likelihood estimate </li>
                      </ul>
                    </li>
                  </ol>
                </li>
              </ul>
            </ul>
          </section>
        </section>
        <section>
          <section>
          <h2>Pose Gesture</h2>
          <img src="media/single_state.jpg"/>
          </section>
          <section>
            <h2>Pose Gesture</h2>
            <ul>
              <li>Training strategies
              <ul>
                <li>Expectation-maximiation for GMM</li>
                <li>Use Bayesian Information Criterion (BIC) to choose the number of mixtures</li>
              </ul>
              </li>
            </ul>
          </section>
        </section>
        <section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/phase_hhmm_obs.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/hmms.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/combined_hmm.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/combined_hmm1.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/combined_hmm2.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/combined_hmm3.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/combined_hmm4.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Combining and Flattening HMM</h2>
          <img src="media/combined_hmm5.jpg"/>
          </section>
        </section>
        <section>
          <section data-transition="none">
            <h2>Online Inference: Fixed-Lag Smoothing</h2> 
            <img src="media/classification.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Online Inference: Fixed-Lag Smoothing</h2>
            <img src="media/filtering.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Online Inference: Fixed-Lag Smoothing</h2>
            <img src="media/fixed_lag.jpg"/>
          </section>
        </section>
        <section>
          <section data-transition="none">
            <h2>Fixed-Lag Smoothing</h2>
            <img src="media/forward1.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Fixed-Lag Smoothing</h2>
            <img src="media/forward2.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Fixed-Lag Smoothing</h2>
            <img src="media/forward3.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Fixed-Lag Smoothing</h2>
            <img src="media/backward.jpg"/>
          </section>
          <section data-transition="none">
            <h2>Fixed-Lag Smoothing</h2>
            <img src="media/gamma.jpg"/>
          </section>
        </section>
        <section data-transition="none">
          <h2>Fixed-Lag Smoothing</h2>
          <ul>
            <li>Time complexity: \(O(H^2l)\)</li>
            <li>Space complexity: \(O(Hl)\)</li>
          </ul>
        </section>
        <section>
          <section data-transition="none">
            <img src="media/dataset.JPG"/>
          </section>
          <section data-transition="none">
            <video muted preload="auto" controls data-autoplay>
            <source src="media/video/dataset.mp4" type="video/mp4"> 
            </video>
          </section>
        </section>
        <section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Discrete Flow</h2>
          <img src="media/circle1.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Discrete Flow</h2>
          <img src="media/circle2.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Discrete Flow</h2>
          <img src="media/circle3.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Discrete Flow</h2>
          <img src="media/circle4.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Discrete Flow</h2>
          <img src="media/circle5.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Discrete Flow</h2>
          <img src="media/circle6.jpg"/>
          </section>
        </section>
        <section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Continuous Flow</h2>
          <img src="media/point1.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Continuous Flow</h2>
          <img src="media/point2.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Continuous Flow</h2>
          <img src="media/point3.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Simultaneous Segmentation and Recognition - Continuous Flow</h2>
          <img src="media/point4.jpg"/>
          </section>
        </section>
        <section>
          <h2>Evaluation</h2>
          <ul>
            <li>Hybrid performance metrics
              <ul>
                <li>Event based metric for discrete flow gestures</li>
                <li>Frame based metric for continuous flow gestures</li>
              </ul>
            </li>
            <li>Average \(F_1\) for online recognition: 80.5%</li>
            <li>Average response time: 0.3s before coming to rest</li>
          </ul>
        </section>
        <section>
          <section data-transition="none">
          <h2>Compare Different Topologies</h2>
          <img src="media/topology_diff.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Compare Different Topologies</h2>
          <img src="media/topology.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Compare Different Topologies</h2>
          <img src="media/topology2_diff.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Compare Different Topologies</h2>
          <img src="media/topology2.jpg"/>
          </section>
          <section data-transition="none">
          <h2>Compare Different Topologies</h2>
          <img src="media/topology3.jpg"/>
          </section>
        </section>
        <section>
          <h2>Effect of Lag Time</h2>
          <img src="media/f1_lag.png"/>
        </section>
        <section>
          <h2>Extensibility</h2>
          <ul>
            <li>Fast training time</li>
            <li>Easy to add new gestures</li>
          </ul>
        </section>
        <section>
          <h2>Future Work</h2>
          <ul>
            <li>Two-hand gestures</li>
            <li>Improve pose gesture recognition</li>
            <li>Improve user independent base model</li>
            <li>Combine different sensors</li>
          </ul>
        </section>
        <section>
        <h2>Contributions</h2>
        <ul>
          <li>Taxonomy-aware HHMM framework
            <ul>
              <li>Probabilistic model that unifies recognition of two forms of gestures</li>
              <li>Hidden state -&gt; gesture phase -&gt; response timing</li>
              <li>Achieves a balance between accuracy and responsiveness</li>
            </ul>
          </li>
          <li>Natural gesture interaction
            <ul>
              <li>Smoothly handles <em>path</em> and <em>pose</em> gestures</li>
              <li>Responds to <em>discrete</em> and <em>continous</em> flow gestures appropriately</li>
            </ul>
          </li>
        </ul>
        </section>
        <section>
          <h2>Acknowledgements</h2>
          <ul>
            <li>Prof. Randall Davis</li>
            <li>Prof. Antonio Torralba and Prof. Bill Freeman</li>
            <li>Group members: Andrew, Jeremy, Yale, Kai, William, Nira, Aaron, Chih-yu, Sonya, Tom</li>
            <li>Friends and family</li>
          </ul>
        </section>
      </div>
    </div>
    <script src="js/reveal.min.js"></script>
    <script src="js/head.min.js"></script>
    <script>
      Reveal.initialize({
        slideNumber: true,

        math: {
          mathjax: "plugin/math/MathJax.js",
          config: "TeX-AMS_HTML"
        },

        dependencies: [
          { src: "plugin/math/math.js", async: true }
        ],

        gesture: {
          mirror: false,
          autoCenter: true
        }
      });
    </script>
  </body>
</html>
